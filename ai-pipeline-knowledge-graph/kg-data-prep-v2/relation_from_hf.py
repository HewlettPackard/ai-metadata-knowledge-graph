###
# Copyright (2024) Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
###

"""
This python script generates relationships for pipeline components from huggingface.
Dependency files: csv files generated by nodes_from_hf.py, files stored at SRC_FOLDER (refer nodes_from_hf.py)
Output: csv files for all relationships with two colums. Files are stored at DEST_LOCAL_PATH and DEST_LUSTRE_PATH

NOTE:
We have lot of model info and dataset info in HF. Only few of those datasets were used to train few of the models.
Therefore, we have a lot of individual dataset nodes but a few relationships with artifacts of a pipeline.
Each pipeline in huggingface is created based on the model. Specifically, model+task.
So models will connections to artifacts but not all dataset will have connections to models

"""
import csv
import pandas as pd
import os
import hashlib

# Source data path
SRC_DATA_PATH = 'data/huggingface'

NODE_CSV_PATH = 'kg-data/huggingface/nodes'

# CSV files created will be stored in both the places
DEST_LUSTRE_PATH = 'kg-data/huggingface/relationships'
DEST_LOCAL_PATH = 'kg-data/huggingface/relationships'



def tuple_to_csv(headers, tuples_list, filepaths):
    """
    headers: column names of csv file to be generated. Type: list of strings
    tuple_list: list of tuples, where each tuple is a row
    filepath: destination filepath of the csv file to be saved
    """
    for filepath in filepaths:
        with open(filepath,'w') as out:
            csv_out=csv.writer(out)
            csv_out.writerow(headers)
            for row in tuples_list:
                csv_out.writerow(row)
        print("File saved at:", filepath)



# Task-pipeline
# src_id in task, src_task_id from pipeline
def rel_pipeline_task():
    task_data = pd.read_csv((os.path.join(NODE_CSV_PATH, 'tasks.csv')))
    pipeline_data = pd.read_csv((os.path.join(NODE_CSV_PATH, 'pipelines.csv')))
    tuples_list = []
    # create a temp dict that maps src_task_id to UUID generated task id
    # Key: source task id, Value: UUID generated task id
    task_id_map = {}
    for i in range(0,len(task_data)):
        task_id_map[str(task_data['src_id'][i])] = str(task_data['task_id'][i])

    for idx in range(0,len(pipeline_data)):
        pipeline_id = pipeline_data['pipeline_id'][idx]
        src_task_id = pipeline_data['src_task_id'][idx]
        if src_task_id == "none":
            pass
        else:
            task_id = task_id_map[str(src_task_id)]
            tuples_list.append((pipeline_id, task_id))
    
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-pipeline-task.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-pipeline-task.csv')
    tuple_to_csv(headers=['pipeline_id', 'task_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])

# pipeline-report
# open model_arxiv.csv
# connect src_model_id in pipeline - src_model_id:arxiv_id - arxiv_id in report
def rel_pipeline_report():
    pipeline_data = pd.read_csv((os.path.join(NODE_CSV_PATH, 'pipelines.csv')))
    model_arxiv_data = pd.read_csv(os.path.join(SRC_DATA_PATH, 'model_arxiv.csv'))
    report_data = pd.read_csv((os.path.join(NODE_CSV_PATH, 'reports.csv')))
    tuples_list = []

    # create tuples of model-arxiv_id
    model_arxiv_tuples = []
    for i in range(0,len(model_arxiv_data)):
        model_arxiv_tuples.append((str(model_arxiv_data['model_id'][i]), str(model_arxiv_data['arxiv_id'][i])))


    # create a temp list of arxiv_id to UUID generated report ID
    # Key: arxiv id, value: UUID generated report id
    arxiv_report_map = {}
    for j in range(0, len(report_data)):
        arxiv_id = str(report_data['arxiv_id'][j])
        arxiv_report_map[arxiv_id] = str(report_data['report_id'][j])
    

    #pipeline - src_model id, src_model_id to arxiv_id, arxiv_id to report id
    # create a temp dic to src_model_id to generated pipeline_id
    model_pipeline_map = {}
    for idx in range(0,len(pipeline_data)):
        pipeline_id = str(pipeline_data['pipeline_id'][idx])
        src_model_id = str(pipeline_data['src_model_id'][idx])
        model_pipeline_map[src_model_id] = pipeline_id
    
    counter = 0
    for pair in model_arxiv_tuples:
        model_id = str(pair[0])
        arxiv_id = str(pair[1])
        try:
            tuples_list.append((model_pipeline_map[model_id], arxiv_report_map[arxiv_id]))
        except KeyError:
            counter = counter + 1
            pass
    
    print("KeyError:" + str(counter) + '/' + str(len(tuples_list)))
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-pipeline-report.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-pipeline-report.csv')
    tuple_to_csv(headers=['pipeline_id', 'report_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])



# pipeline-framework
def rel_pipeline_framework():
    framework_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'frameworks.csv'))

    tuples_list = []
    for idx in range(0, len(framework_data)):
        tuples_list.append((str(framework_data['pipeline_id'][idx]), str(framework_data['framework_id'][idx])))


    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-pipeline-framework.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-pipeline-framework.csv')
    tuple_to_csv(headers=['pipeline_id', 'framework_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])


# pipeline-stage
def rel_pipeline_stage():
    stage_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'stages.csv'))

    tuples_list = []
    for idx in range(0, len(stage_data)):
        tuples_list.append((str(stage_data['pipeline_id'][idx]), str(stage_data['stage_id'][idx])))

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-pipeline-stage.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-pipeline-stage.csv')
    tuple_to_csv(headers=['pipeline_id', 'stage_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])



# stage-execution
def rel_stage_execution():
    execution_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'executions.csv'))

    tuples_list = []
    for idx in range(0, len(execution_data)):
        tuples_list.append((str(execution_data['stage_id'][idx]), str(execution_data['execution_id'][idx])))

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-stage-execution.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-stage-execution.csv')
    tuple_to_csv(headers=['stage_id', 'execution_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])
  


def rel_execution_artifact():
    artifact_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'artifacts.csv'))

    tuples_list = []
    for idx in range(0, len(artifact_data)):
        tuples_list.append((str(artifact_data['execution_id'][idx]), str(artifact_data['artifact_id'][idx]) ))

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-execution-artifact.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-execution-artifact.csv')
    tuple_to_csv(headers=['execution_id', 'artifact_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])



# Artifact-dataset
# We have lot of models and datasets in huggignface. Only some of them have connections
def rel_artifact_dataset():
    artifact_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'artifacts.csv'))
    dataset_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'datasets.csv'))

    artifact_dataset_map = {}
    # a temp dict of gen_artifact_id and src_dataset_id 
    for i in range(0, len(artifact_data)):
        src_dataset_id = str(artifact_data['src_dataset_id'][i])
        artifact_id = str(artifact_data['artifact_id'][i])
        artifact_dataset_map[artifact_id] = src_dataset_id
    

    # a temp dict of src_dataset_id and generated dataset_id
    dataset_map = {}
    for j in range(0,len(dataset_data)):
        src_dataset_id = str(dataset_data['src_id'][j])
        dataset_id = str(dataset_data['dataset_id'][j])
        dataset_map[src_dataset_id] = dataset_id
    
    tuples_list = []
    for artifact_id in artifact_dataset_map:
        src_dataset_id = artifact_dataset_map[artifact_id]
        try:
            tuples_list.append((artifact_id, dataset_map[src_dataset_id]))
        except KeyError:
            pass
    
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-artifact-dataset.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-artifact-dataset.csv')
    tuple_to_csv(headers=['artifact_id', 'dataset_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])   
    


def rel_artifact_model():
    artifact_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'artifacts.csv'))
    model_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'models.csv'))

    # create a temp dict of source model id to UUID generated model id
    # Key: source model id, Value: UUID generated model id
    model_id_map = {}
    for i in range(0,len(model_data)):
        model_id_map[str(model_data['src_id'][i])] = str(model_data['model_id'][i])
    
    tuples_list = []
    for idx in range(0,len(artifact_data)):
        artifact_id = artifact_data['artifact_id'][idx]
        model_id = model_id_map[str(artifact_data['src_model_id'][idx])]
        tuples_list.append((artifact_id, model_id))

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-artifact-model.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-artifact-model.csv')
    tuple_to_csv(headers=['artifact_id', 'model_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])  




# # Helper function
def create_uuid_from_string(string):
    hex_string = int(hashlib.sha1(string.encode("utf-8")).hexdigest(), 16) % (10 ** 10)
    return str(hex_string)


def rel_artifact_metric():
    print("Relation - artifact - metric...")
    artifact_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'artifacts.csv'))
    metric_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'metrics.csv'))

    artifact_to_tuple = {}
    for i in range(0,len(artifact_data)):
        artifact_id = artifact_data['artifact_id'][i]
        src_model_id = artifact_data['src_model_id'][i]
        src_dataset_id = artifact_data['src_dataset_id'][i]
        artifact_to_tuple[artifact_id] = (src_model_id, src_dataset_id)
    
    metric_to_tuple = {}
    for j in range(0,len(metric_data)):
        metric_id = metric_data['metric_id'][j]
        src_model_id = metric_data['src_model_id'][j]
        src_dataset_id = metric_data['src_dataset_id'][j]
        metric_to_tuple[metric_id] = (src_model_id, src_dataset_id)


    tuples_list = []
    total_len = len(artifact_data)
    for i, artifact_id in enumerate(artifact_to_tuple):
        print(" Rel-Artifact-Metric:" + str(i) + '/' + str(total_len))
        tuple_set1 = artifact_to_tuple[artifact_id]
        for metric_id in metric_to_tuple:
            if metric_to_tuple[metric_id] == tuple_set1:
                # print(metric_to_tuple[metric_id], tuple_set1)
                tuples_list.append((artifact_id, metric_id))
            else:
                pass

    print(len(metric_to_tuple), len(artifact_to_tuple), len(tuples_list))
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-artifact-metric.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-artifact-metric.csv')
    tuple_to_csv(headers=['artifact_id', 'metric_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])  


def rel_execution_metric():
    print("Relation - execution - metric...")
    execution_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'executions.csv'))
    metric_data = pd.read_csv(os.path.join(NODE_CSV_PATH, 'metrics.csv'))

    execution_to_tuple = {}
    for i in range(0,len(execution_data)):
        execution_id = execution_data['execution_id'][i]
        src_model_id = execution_data['src_model_id'][i]
        src_dataset_id = execution_data['src_dataset_id'][i]
        execution_to_tuple[execution_id] = (src_model_id, src_dataset_id)
    
    metric_to_tuple = {}
    for j in range(0,len(metric_data)):
        metric_id = metric_data['metric_id'][j]
        src_model_id = metric_data['src_model_id'][j]
        src_dataset_id = metric_data['src_dataset_id'][j]
        metric_to_tuple[metric_id] = (src_model_id, src_dataset_id)


    tuples_list = []
    total_len = len(execution_data)
    for i, execution_id in enumerate(execution_to_tuple):
        print(" Rel-Execution-Metric:" + str(i) + '/' + str(total_len))
        tuple_set1 = execution_to_tuple[execution_id]
        for metric_id in metric_to_tuple:
            if metric_to_tuple[metric_id] == tuple_set1:
                # print(metric_to_tuple[metric_id], tuple_set1)
                tuples_list.append((execution_id, metric_id))
            else:
                pass

    print(len(metric_to_tuple), len(execution_to_tuple), len(tuples_list))
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'rel-execution-metric.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'rel-execution-metric.csv')
    tuple_to_csv(headers=['execution_id', 'metric_id'], tuples_list=tuples_list, filepaths=[filepath1, filepath2])  





# Function calls
rel_pipeline_task()
rel_pipeline_report() 
rel_pipeline_framework()
rel_pipeline_stage()
rel_stage_execution()
rel_execution_artifact()
rel_artifact_dataset() 
rel_artifact_model()
rel_artifact_metric()
rel_execution_metric()
