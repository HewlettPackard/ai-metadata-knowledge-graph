###
# Copyright (2024) Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
###



import pandas as pd
import os
import uuid
import hashlib
import csv


# Source data path
SRC_DATA_PATH = 'data/open-ml'

# CSV files created will be stored in both the places
DEST_LUSTRE_PATH = 'kg-data/open-ml/nodes'
DEST_LOCAL_PATH = 'kg-data/open-ml/nodes'


category_vocab = ['classification', 'analysis', 'discovery', 'challenge', 'clustering']

# URL - https://www.openml.org/search?type=task_type&sort=date
task_type_description = {'Survival Analysis':'Related to regression. Given a dataset, typically consisting of patient data, predict a left timestamp (date entering study), right timestamp (date of leaving the study) or both.',
                  'Machine Learning Challenge': 'This is a standard machine learning challenge with private dataset',
                  'Clustering': 'Given an input dataset, the task is to partition it into various clusters',
                  'Supervised Data Stream Classification': 'Given a dataset with a nominal target, various data samples of increasing size are defined. A model is build for each individual data sample; from this a learning curve can be drawn',
                  'Learning Curve':'Given a dataset with a nominal target, various data samples of increasing size are defined. A model is build for each individual data sample; from this a learning curve can be drawn',
                  'Supervised Regression': 'Given a dataset with numeric target and a set of train/test splits, eg. generated by cross-validation procedure, train a model and return the predictions of that model',
                  'Supervised Classification': 'In supervised classification, you are given an input dataset in which instances are labeled with a certain class. The goal is to build a model that predicts the class for future unlabeled instances. The model is evaluated using a train-test procedure, eg.cross-validation',
                  'Subgroup Discovery': ''}


runs_folder = os.path.join(SRC_DATA_PATH, 'recent_runs')




# Helper functions
def create_uuid_from_string(string):
    hex_string = int(hashlib.sha1(string.encode("utf-8")).hexdigest(), 16) % (10 ** 10)
    return str(hex_string)


def write_to_csv(data, filepaths):
    # return 0
    # writing to a csv file
    csv_data = data

    # store at all the filepaths
    for filepath in filepaths:
        csv_data_file = open(filepath, 'w')

        # create the csv writer object
        csv_writer = csv.writer(csv_data_file)

        # Counter variable used for writing
        # headers to the CSV file
        count = 0
        for element in csv_data:
            if count == 0:
                # Writing headers of CSV file
                header = element.keys()
                csv_writer.writerow(header)
                count += 1
            # Writing data of CSV file
            csv_writer.writerow(element.values())
        csv_data_file.close()
        print("File saved at:", filepath)


def compute_dataset_modality(features):
    if features is None:
        return 'none'
    else:
        feature_list = []
        elements = features.split(",")
        for ele in elements:
            value = ele.split(':')[-1]
            temp = value.split("(")[-1]
            temp = temp.split(")")[0]
            feature_list.append(temp)
        # For now, return only one feature by finding the dominant one. 
        # Later on do a set and return all unique features once we figure out how to load multiple values to neo4j
        unique_features = list(set(feature_list))
        dominant_feature = ''
        max_count = 0
        for f in unique_features:
            f_count = feature_list.count(f)
            if f_count>max_count:
                max_count=f_count
                dominant_feature = f
            else:
                pass
        
        return str(dominant_feature)



# Helper function
def process_task_name(task_name, dataset_name):
    task_name = task_name.lower()
    task_name_tokens = task_name.split()
    dataset_name = dataset_name.split('-')
    dataset_tokens = []
    for d in dataset_name:
        d = d.split('_')
        dataset_tokens = dataset_tokens + d

    id_tokens = task_name_tokens + dataset_tokens
    taskID = '-'.join(id_tokens)
    return taskID


# Helper function
def compute_task_category(taskID):
    tokens = taskID.split("-")
    category_list = list(set(tokens).intersection(set(category_vocab)))
    if len(category_list) == 0:
        category = 'none'
    else:
        category = ','.join(category_list)
    return category



def create_pipelines():
    """
    This script creates the following csv files - pipeline.csv, stage.csv, execution.csv, artifact.csv

    Columns such as flow id, dataset id and etc are included to create relationships
    """

    filenames = os.listdir(runs_folder)
    pipeline_data_list = []
    stage_data_list = []
    execution_data_list = []
    artifact_data_list = []
    metric_data_list = []
    parameter_data_list = []
    report_data_list = []

    counter = 0

    for f in filenames:
        filepath = os.path.join(runs_folder, f)
        df = pd.read_csv(filepath)
        total_length = len(df)
        for i in range(0,len(df)):
            counter = counter + 1
            print("Preparing pipeline, stage, execution, artifact, parameters and metrics nodes. Counter:" + str(counter) + '/' + str(total_length))

            try:
                run_id = df['run_id'][i]
                task_id = df['task_id'][i]
                dataset_id = df['dataset_id'][i]
                flow_id = df['flow_id'][i]
                dataset_name = df['name'][i]

                # Pipeline construction
                pipeline_name = str(df['task_type'][i]) + ' on ' + str(dataset_name) + ' with ' + str(df['flow_name'][i])
                string = pipeline_name
                pipeline_id = create_uuid_from_string(string)
                
                pipeline_data = {'pipeline_id': pipeline_id,
                        'pipeline_name': pipeline_name,
                        'source': 'openml',
                        'src_id': run_id, # it should be flow or something
                        'openml_flow_id': flow_id,
                        'openml_task_id': task_id,
                        'openml_dataset_id': dataset_id,
                        'openml_run_id': run_id
                }
                pipeline_data_list.append(pipeline_data)


                # Stage construction
                stage_name = 'train-test'
                stage_string = pipeline_name + 'stage' + str(stage_name)
                stage_id = create_uuid_from_string(stage_string)

                stage_data = {'stage_id': stage_id,
                            'stage_name': stage_name,
                            'pipeline_id': pipeline_id,
                            'pipeline_name': pipeline_name,
                            'openml_run_id': run_id,
                            'source': 'openml',
                            'properties': ""
                }
                stage_data_list.append(stage_data)

                # Execution construction
                execution_string = pipeline_name + 'execution' + stage_name + str(run_id)
                execution_id = create_uuid_from_string(execution_string)
                execution_name = pipeline_name + '-' + str(stage_name)
                execution_data = {'execution_id': execution_id,
                                'execution_name': execution_name,
                                'pipeline_id': pipeline_id,
                                'pipeline_name': pipeline_name,
                                'stage_id': stage_id,
                                'openml_run_id': run_id,
                                'source': 'openml',
                                'command': '',
                                'properties': ""

                }
                execution_data_list.append(execution_data)
    
                # Artifact construction
                # needs - execution id, model id (flow_id), dataset_id (dataset_id)
                artifact_string = pipeline_name + 'artifact' + str(run_id)
                artifact_id = create_uuid_from_string(artifact_string)
                artifact_name = pipeline_name + " - artifacts"
                artifact_data = {'artifact_id': artifact_id, 
                                'artifact_name': artifact_name,
                                'pipeline_id': pipeline_id,
                                'pipeline_name': pipeline_name,
                                'source': 'openml',
                                'execution_id': execution_id,
                                'openml_dataset_id': dataset_id,
                                'openml_flow_id': flow_id,
                                'openml_run_id': run_id
                }
                artifact_data_list.append(artifact_data)

                # Metric construction
                metric_string = pipeline_name + 'metrics' + str(run_id)
                metric_id = create_uuid_from_string(metric_string)

                # TODO -add predictions url as url results

                metric_data = {'metric_id': metric_id,
                               'metrics': df['predictions_url'][i], # there are a lot of metrics. So including url for now
                            'pipeline_id': pipeline_id,
                            'pipeline_name': pipeline_name,
                            'source': 'openml',
                            'artifact_id': artifact_id,
                            'execution_id': execution_id,
                            'openml_run_id': run_id,
                            'prediction_url': df['predictions_url'][i]
                            }

                metric_data_list.append(metric_data)


                # Hyperparameter construction
                parameter_string = pipeline_name + 'parameters' + str(run_id)
                parameter_id = create_uuid_from_string(parameter_string)

                parameter_data = {'openml_run_id': run_id,
                        'openml_task_id': task_id,
                        'openml_dataset_id': dataset_id,
                        'openml_flow_id': flow_id,
                        'parameter_id': parameter_id,
                        'source': 'openml',
                'setup': df['setup_string'][i],
                'settings': df['parameter_settings'][i]
                }
                parameter_data_list.append(parameter_data)

            except KeyError as e:
                print(e)
                pass

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'pipelines.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'pipelines.csv')
    write_to_csv(pipeline_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'stages.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'stages.csv')
    write_to_csv(stage_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'executions.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'executions.csv')
    write_to_csv(execution_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'artifacts.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'artifacts.csv')
    write_to_csv(artifact_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'metrics.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'metrics.csv')
    write_to_csv(metric_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'parameters.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'parameters.csv')
    write_to_csv(parameter_data_list, [filepath1, filepath2])

    
def create_frameworks():
    filenames = os.listdir(runs_folder)
    data_list = []
    counter = 0
    for f in filenames:
        filepath = os.path.join(runs_folder, f)
        df = pd.read_csv(filepath)
        for i in range(0,len(df)):
            counter = counter + 1
            print("Creating Frameworks. Counter:", counter)
            framework_list = df['tags'][i]
            if isinstance(framework_list, str):
                framework_name = framework_list
                run_id = df['run_id'][i]
                framework_id = str(create_uuid_from_string(framework_name))
                desc = 'This pipeline was built using the frameworks - ' + str(framework_name)

                data = {'framework_id': framework_id,
                        'framework_name': framework_name,
                        'framework_version': df['external_version'][i],
                        'openml_run_id': run_id,
                        'source': 'openml',
                        'url': '',
                        'description':desc
                }
                data_list.append(data)
        # exit(0)
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'frameworks.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'frameworks.csv')
    # Call write_to_csv
    write_to_csv(data_list, [filepath1, filepath2])


def create_datasets():
    """
    all_datastes.xlsx
    'qualities' have number of features, number of instances and etc
    'features'  have detailed features
    """
    filepath = os.path.join(SRC_DATA_PATH, 'data/open-ml/all_datasets.xlsx')
    df = pd.read_excel(filepath)
    print("Creating datasets..")
    data_list = []
    dataset_modality_map = {}
    for i in range(0,len(df)):
        dataset_name = df['name'][i]
        openml_dataset_id = df['dataset_id'][i]
        features = df['features'][i]
        computed_modality = compute_dataset_modality(features)
        desc = str(df['description'][i])
        url = str(df['original_data_url'][i])
        dataset_id = create_uuid_from_string(dataset_name)

        if desc == 'None' or desc == 'nan':
            desc = 'none'
        if url == 'None' or url == 'nan':
            url = 'none'

        data = {'dataset_id': dataset_id,
                    'dataset_name': dataset_name,
                    'modality': str(computed_modality),
                    'description': str(desc),
                    'url': str(url),
                    'source': 'openml',
                    'openml_id': openml_dataset_id,
                    'src_id': openml_dataset_id
        }

        data_list.append(data)
        dataset_modality_map[dataset_name] = computed_modality
    
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'datasets.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'datasets.csv')
    # Call write_to_csv
    write_to_csv(data_list, [filepath1, filepath2])

    return dataset_modality_map



def create_tasks(dataset_modality_map):
    filepath = os.path.join(SRC_DATA_PATH, 'data/open-ml/all_tasks.csv')
    df = pd.read_csv(filepath)
    print("Creating tasks..")
    data_list=[]
    for i in range(0, len(df)):
        task_type = str(df['task_type'][i])
        dataset_name = df['name'][i]
        dataset_id = df['did'][i]
        openml_task_id = df['tid'][i]
        task_name = task_type + ' on ' + dataset_name
        
        task_id = create_uuid_from_string(task_name)
        processed_task_name = process_task_name(task_name, dataset_name)
        category = compute_task_category(processed_task_name)

        if task_type == 'None' or task_type == 'nan':
            task_type_desc = 'none'
        else:
            task_type_desc = task_type_description[task_type]
        
        try:
            modality = dataset_modality_map[str(dataset_name)]
        except KeyError:
            modality = 'none'
        

        # TODO - do we have task description?
        data = {'task_id': task_id,
                'task_name': task_name,
                'task_type': str(task_type),
                'category': str(category),
                'modality': str(modality),
                'description': str(task_type_desc),
                'source': 'openml',
                'src_id': openml_task_id,
                'openml_dataset_id': dataset_id}

        data_list.append(data)
    
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'tasks.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'tasks.csv')
    # Call write_to_csv
    write_to_csv(data_list, [filepath1, filepath2])



def create_models():
    filepath = os.path.join(SRC_DATA_PATH, 'data/open-ml/flow_details.xlsx')
    # Check flows.xlsx inside flows folder
    df = pd.read_excel(filepath)
    print("Creating models..")
    data_list = []
    for i in range(0,len(df)):
        flow_id = df['flow_id'][i]
        model_name = df['name'][i]
        description = str(df['description'][i])
        model_class = str(df['class_name'][i])
        model_id = create_uuid_from_string(str(model_name))

        if model_class is None or model_class == 'nan':
            model_class = 'none'
        if description is None or description == 'nan':
            description = 'none'


        data = { 'openml_id': flow_id,
                'model_id': model_id,
                'model_name': model_name,
                'model_class': str(model_class),
                'url': 'none',
                'description': str(description),
                'source': 'openml',
                'src_id': flow_id
        }

        data_list.append(data)

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'models.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'models.csv')
    # Call write_to_csv
    write_to_csv(data_list, [filepath1, filepath2])

        

# Function calls
dataset_modality_map = create_datasets()
create_tasks(dataset_modality_map)
create_models()
create_pipelines() # creates nodes for pipeline, report, stages, artifact, metric and report
create_frameworks()

