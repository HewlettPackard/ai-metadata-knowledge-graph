###
# Copyright (2024) Hewlett Packard Enterprise Development LP
#
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
###


"""
This script creates csv files for nodes to be loaded in to the knowlege graph. These files (nodes) are created by 
mapping PWC data nomenclature to the proposed metadata ontology nomenclature. Also, it computes additional property 
values for the concepts.

Dependent on the csv files (../data/pwc/csv) generated by python script - ../papers-with-code/json_csv.py

Mapping of nodes:
papers --> pipelines
methods --> models
Rest of the node names remain the same
"""


import pandas as pd
import os
import csv
import json
import re
import inflect
import hashlib
import uuid
# from itertools import take

p = inflect.engine()

task_category_vocab = ['recognition', 'regression','reconstruction', 'segmentation', 'detection', 'generation', 'harmonization', 'translation', 'classification', 'adaptation', 'search', 'analysis',
'extraction', 'retrieval', 'annotation', 'generalization', 'augmentation', 'anonymization', 'prediction', 'correlation', 'fusion', 'matching', 'synthesis', 'understanding',
'testing', 'parsing', 'identification', 'transfer', 'spotting', 'estimation', 'resolution', 'clustering', 'separation', 'localization', 'summarization', 'reccommendation',
'expansion', 'labeling', 'imaging', 'interpretation', 'captioning', 'retrieval', 'selection', 'assessment', 'registration', 'forecasting', 'planning', 'tracking', 'inference',
'grounding', 'disambiguation', 'reasoning', 'comprehension', 'reading', 'reduction', 'completion', 'compression', 'decomposition', 'learning', 'sampling', 'verification', 'animation',
'interpolation', 'visualizaiton', 'propagation', 'mining', 'surveillance', 'diagnosis', 'ranking', 'optimization', 'synthesis', 'anomaly', 'linking']

# Task Modality vocabulary
image_vocab = ['2d', '3d', 'image', 'visual', 'depth', 'pixel', 'voxel', 'RBG', 'action', 'object', 'facial',
'pose', 'grayscale', 'texture', 'pattern','face', 'scene', 'imagery', 'imaging', 'image-based', 'vision', 'computer-vision', 'computer vision']
text_vocab = ['text', 'word', 'language', 'lingual', 'dialogue', 'dialog', 'corpus', 'sentence', 'reading', 'news', 'reviews', 
'grammar', 'grammatical', 'natural-language-processing', 'nlp', 'natural language processing',
'textual', 'translation', 'question', 'answering', 'conversational', 'conversation', 'entity', 'document', 'paragraph', 'paraphrase']
video_vocab = ['video', 'video-based', 'motion']
audio_vocab = ['audio', 'voice', 'speech', 'sound', 'headphone', 'music', 'spoken']
multi_vocab = ['multi', 'cross', 'multimodal', 'crossmodal', 'multi-modal']
super_class = ['image', 'text', 'video', 'audio']


# Source data path
SRC_DATA_PATH = 'data/pwc'

# CSV files created will be stored in both the places
DEST_LUSTRE_PATH = 'kg-data/pwc/nodes'
DEST_LOCAL_PATH = 'kg-data/pwc/nodes'



"""
NOTES:

ID generations from string

task --> from: task id of pwc
dataset --> from: dataset id of pwc
model --> from: method id of pwc
pipeline --> from: 'pipeline' + paper title
report --> from: paper title

stage --> from: pipeline_name + 'stage' + str(stage_name)
execution --> from: pipeline_name + 'execution' + str(openml_run_id)
artifact --> from: pipeline_name + 'artifact' + str(openml_run_id)
metrics --> from: result id of pwc
frameworks --> from: git url

"""



# Helper functions
def create_uuid_from_string(string):
    hex_string = int(hashlib.sha1(string.encode("utf-8")).hexdigest(), 16) % (10 ** 10)
    return str(hex_string)


def write_to_csv(data, filepaths):
    # writing to a csv file
    csv_data = data


    for filepath in filepaths:
        csv_data_file = open(filepath, 'w')
        # create the csv writer object
        csv_writer = csv.writer(csv_data_file)

        # Counter variable used for writing
        # headers to the CSV file
        count = 0
        for element in csv_data:
            if count == 0:
                # Writing headers of CSV file
                header = element.keys()
                csv_writer.writerow(header)
                count += 1
            # Writing data of CSV file
            csv_writer.writerow(element.values())
        csv_data_file.close()
        print("File saved at:", filepath)



# TODO - need to figure out how to load multiple values to single property in neo4j. For now, loading the first element of
# identified modality and category

def split_task_names(task_name):
    """
    INPUT: task_name --> name of the task in the format of string
    OUPUT: a list of words obtained form the task name
    """
    # strip the brackets
    task_name = re.sub(r"[\([{})\]]", "", task_name)
    task_name = task_name.lower()
    # split based on space
    split_name = task_name.split(" ")     
    # split based on '-' if any
    proc_task_names = []
    for name in split_name:
        n = name.split("-")
        proc_task_names = proc_task_names + n
    return proc_task_names


def convert_singular(task_name_tokens):
    """
    INPUT: task_name_list --> a list of words obtained form the task name
    OUTPUT: a list with a singular form of words present in the INPUT
    """
    singular = []
    for w in task_name_tokens:
        x = p.singular_noun(w)
        if x:
            singular.append(x)
        else:
            singular.append(w)
    return singular


def process_task_titles(task_name):
    task_name_tokens = split_task_names(task_name)
    task_name_singular = convert_singular(task_name_tokens)
    return task_name_singular


def tokenize(item):
    tokens = item.split(" ")
    return tokens


def compute_category(item_tokens):
    tokens = list(item_tokens)
    inter = list(set(tokens).intersection(set(task_category_vocab)))
    if len(inter) > 0:
        category = ','.join(inter)
    else:
        category = "none"
    return category


def compute_modality(item_tokens):
    tokens = list(item_tokens)
    modality_list = []
    # check for images
    if len(set(tokens).intersection(set(image_vocab))) > 0:
        modality_list.append('image')

    # check for text
    elif len(set(tokens).intersection(set(text_vocab))) > 0:
        modality_list.append('text')

    # check for audio
    elif len(set(tokens).intersection(set(audio_vocab))) > 0:
        modality_list.append('audio')
    
    # check for video
    elif len(set(tokens).intersection(set(video_vocab))) > 0:
        modality_list.append('video')
    
    # check for multimodal
    elif len(set(tokens).intersection(set(multi_vocab))) > 0:
        modality_list.append('multimodal')
    else:
        pass
    
    # If title has image and text but not multimodal or cross modal, the following case will capture it
    if len(set(modality_list).intersection(set(super_class))) > 1:
        modality_list.append('multimodal')
    
    if len(modality_list) > 0:
        modality = ','.join(modality_list)
    else:
        modality = "none"
    return modality

# modality based on term frequency
def tf_modality(tokens):
    unique_words = set(tokens)
    tf_dict = {}
    for w in unique_words:
        freq = tokens.count(w)
        tf_dict[w] = freq
    modality_terms = []
    modality_tf = []
    # check for images
    image_terms = list(set(tokens).intersection(set(image_vocab)))
    if len(image_terms)> 0:
        for term in image_terms:
            modality_terms.append('image')
            modality_tf.append(tf_dict[term])
    
    # check for text
    text_terms = list(set(tokens).intersection(set(text_vocab)))
    if len(text_terms)> 0:
        for term in text_terms:
            modality_terms.append('text')
            modality_tf.append(tf_dict[term])

    # check for video
    video_terms = list(set(tokens).intersection(set(video_vocab)))
    if len(video_terms)> 0:
        for term in video_terms:
            modality_terms.append('video')
            modality_tf.append(tf_dict[term])

    # check for audio
    audio_terms = list(set(tokens).intersection(set(audio_vocab)))
    if len(audio_terms)> 0:
        for term in text_terms:
            modality_terms.append('audio')
            modality_tf.append(tf_dict[term])    

    # check for multimodal
    multimodal_terms = list(set(tokens).intersection(set(multi_vocab)))
    if len(multimodal_terms)> 0:
        for term in multimodal_terms:
            modality_terms.append('multimodal')
            modality_tf.append(tf_dict[term])  
    
    if len(modality_terms) == 0:
        return "none"
    else:
        max_freq = max(modality_tf)
        max_freq_idx = modality_tf.index(max_freq)
        term = modality_terms[max_freq_idx]
        return ",".join(modality_terms)



def process_pipeline_name(raw_pipeline_name):
    paper_name = raw_pipeline_name.replace('\"', " ")
    paper_name = paper_name.replace("\'", " ")
    paper_name = paper_name.replace(",", "")
    paper_name = paper_name.replace("\\", "")
    # paper_name = re.sub(r"[\([{,})\]]", "", paper_name)
    return paper_name

def create_nodes():
    filepath = os.path.join(SRC_DATA_PATH, 'all-papers.json')
    paper_data = json.load(open(filepath, 'r'))
    counter = 0

    pipeline_data_list = []
    report_data_list = []
    stage_data_list = []
    execution_data_list = []
    artifact_data_list = []

    for i in paper_data:
        paper_id = paper_data[i]['id']
        counter = counter + 1
        print("Creating nodes: Pipeline, stage, execution and report. Counter:", counter)

        abstract = str(paper_data[i]['abstract'])
        abstract = abstract.replace(",", "")
        abstract = abstract.replace("\\", "")
        abstract_tokens_ = tokenize(abstract)
        abstract_tokens = [t.lower() for t in abstract_tokens_]

        pipeline_name = process_pipeline_name(paper_data[i]['title'])
        pipeline_name_tokens_ = tokenize(pipeline_name)
        pipeline_name_tokens = [t.lower() for t in pipeline_name_tokens_]


        # Computing modality for pipelines that can be used for tasks and datasets
        # Compute using title first. If that is none, then compute using abstract
        modality = compute_modality(pipeline_name_tokens)
        if modality == "none":
            modality = tf_modality(abstract_tokens) # for paragraphs, use term-frequency based modality

        # pipeline node construction
        pipeline_string = 'pipeline' + str(pipeline_name)
        pipeline_id = create_uuid_from_string(pipeline_string)
        pipeline_data = {'pipeline_id': pipeline_id,
                        'pipeline_name': pipeline_name,
                        'modality': modality, #used in post processing.
                        'source': 'papers-with-code',
                        'src_id': str(paper_id),
                }
        pipeline_data_list.append(pipeline_data)

        # Report node construction
        # Not loading abstract text as the special characters in the text messess with csv format and so loading the data to Neo4j.
        # If any data extraction needs to be done on the abstract, it needs to be done before and only the values to should be loaded to csv.
        url = paper_data[i]['url_pdf']
        report_id = create_uuid_from_string(url)
        paper_title = process_pipeline_name(paper_data[i]['title'])

        report_data = {'report_pdf_url': str(url),
                        'report_id': report_id,
                        'title': paper_title,
                        'abstract_url': paper_data[i]['url_abs'],
                        'abstract': abstract,
                        "source": 'papers-with-code',
                        'src_id': paper_id,
                        'pipeline_id': pipeline_id
        }
        report_data_list.append(report_data)
    
        # Stage construction
        stage_name = 'test'
        stage_string = str(pipeline_name) + 'stage' + stage_name
        stage_id = create_uuid_from_string(stage_string)
        stage_data = {'stage_id': stage_id,
                            'stage_name': stage_name,
                            'source': 'papers-with-code',
                            'pipeline_id': pipeline_id,
                            'pipeline_name': pipeline_name,
                            'properties': ""
                }
        stage_data_list.append(stage_data)
        
        # Execution construction
        execution_name = pipeline_name + '-' + stage_name
        execution_string = pipeline_name + 'execution' + stage_name
        execution_id = create_uuid_from_string(execution_string)
        execution_data = {'execution_id': execution_id,
                                'execution_name': execution_name,
                                'pipeline_id': pipeline_id,
                                'pipeline_name': pipeline_name,
                                'stage_id': stage_id,
                                'source': 'papers-with-code',
                                'pwc_paper_id': paper_id, # To construct relationships
                                'command': 'none',
                                'properties': 'none'                            
                                }
        execution_data_list.append(execution_data)
        
        # Artifact Construction
        artifact_string = pipeline_name + 'artifact' + str(execution_id)
        artifact_name = str(pipeline_name) + ' - artifacts'
        artifact_id = create_uuid_from_string(artifact_string)
        artifact_data = {'artifact_id': artifact_id, 
                                'artifact_name': artifact_name,
                                'pipeline_id': pipeline_id,
                                'pipeline_name': pipeline_name,
                                'pwc_paper_id': paper_id, # To construct relationships
                                'source': 'papers-with-code',
                                'execution_id': execution_id,
                }
        artifact_data_list.append(artifact_data)

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'pipelines.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'pipelines.csv')
    write_to_csv(pipeline_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'stages.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'stages.csv')
    write_to_csv(stage_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'executions.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'executions.csv')
    write_to_csv(execution_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'artifacts.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'artifacts.csv')
    write_to_csv(artifact_data_list, [filepath1, filepath2])

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'reports.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'reports.csv')
    write_to_csv(report_data_list, [filepath1, filepath2])

    

def create_tasks():
    src_filename = os.path.join(SRC_DATA_PATH,'pwc-tasks.json')
    tasks_data = json.load(open(src_filename, 'r'))
    task_data_list = []
    modality_counter = 0
    for tid in tasks_data:
        task_id = create_uuid_from_string(str(tid))
        task_name = tasks_data[tid]['name']
        description = tasks_data[tid]['description']
        if description is None:
            description = 'none'

        task_name_tokens = tid.split("-")
        modality = compute_modality(task_name_tokens)
        category = compute_category(task_name_tokens)

        # if modality cannot be captured from name, use description to capture it
        if modality == "none" and description != "none":
            desc_tokens = tokenize(description)
            modality = tf_modality(desc_tokens) # computes based on term frequency

        if modality != "none":
            modality_counter += 1

        data = {'task_id': task_id,
        'task_name': task_name,
        'task_type': 'none',
        'category': str(category),
        'modality': str(modality),
        'description': str(description),
        'source': 'papers-with-code',
        'src_id': str(tid)}

        task_data_list.append(data)

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'tasks.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'tasks.csv')
    write_to_csv(task_data_list, [filepath1, filepath2])
    


def create_models():
    src_filename = os.path.join(SRC_DATA_PATH, 'pwc-methods.json')
    models_data = json.load(open(src_filename, 'r'))
    model_data_list = []

    for mid in models_data:
        src_id = models_data[mid]['id']
        model_name = models_data[mid]['full_name']
        model_id = create_uuid_from_string(model_name)
        description = models_data[mid]['description']

        if description is None:
            description = 'none'

        data = {
            'model_id': model_id,
            'model_name': model_name,
            'model_class': 'none',
            'description': str(description),
            'url': 'none',
            'source': 'papers-with-code',
            'src_id': src_id
        }
        model_data_list.append(data)
    
    print("Len of models data", len(model_data_list))
    
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'models.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'models.csv')
    write_to_csv(model_data_list, [filepath1, filepath2])


def create_datasets():
    src_filename = os.path.join(SRC_DATA_PATH, 'pwc-datasets.json')
    datasets_data = json.load(open(src_filename, 'r'))

    dataset_data_list = []
    for did in datasets_data:
        dataset_name = datasets_data[did]['full_name']
        if dataset_name is None:
            pass
        else:
            src_id = did
            dataset_name = datasets_data[did]['full_name']
            dataset_id = create_uuid_from_string(dataset_name)
            url = datasets_data[did]['url']
            dataset_tokens = tokenize(dataset_name)
            modality = compute_modality(dataset_tokens) # most of this is none. But will be resolved in post processing

            if url is None:
                url = 'none'
            data = {'dataset_id': dataset_id,
                'dataset_name': dataset_name,
                'modality': modality,
                'description': 'none',
                'url': str(url),
                'source': 'papers-with-code',
                'src_id':src_id
            }
            dataset_data_list.append(data)

    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'datasets.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'datasets.csv')
    write_to_csv(dataset_data_list, [filepath1, filepath2])



def create_metrics():
    " Merging evaluations and results of papers-with-code to metrics"
    # TODO - create two columns - best_metric_name, best_metric_values
    src_filename = os.path.join(SRC_DATA_PATH, 'pwc-results.json')
    results_data = json.load(open(src_filename, 'r'))
    # KEYS - id, best_rank, metrics, methodology, uses_additiona_data, paper, best_metric, evaluated_on(date), external_source_url, evaluation_id

    eval_filepath = os.path.join(SRC_DATA_PATH, 'pwc-evaluations.json')
    evaluations_data = json.load(open(eval_filepath, 'r'))
    # KEYS - id, task, dataset, description, mirror_url
    print(len(results_data), len(evaluations_data))
    metrics_data_list = []
    for rid in results_data:
        eval_id = results_data[rid]['evaluation_id']
        metric_id = create_uuid_from_string(str(rid))

        try:
            pwc_dataset_id = evaluations_data[eval_id]['dataset']
            pwc_task_id = evaluations_data[eval_id]['task']
        except KeyError as e:
            # filled in random value that possibly will not be an ID. So that the graph database will not add an edge
            pwc_dataset_id = 'none'
            pwc_task_id = 'none'
            pass
        

        data = {'metric_id': metric_id,
                    'metrics': str(results_data[rid]['metrics']),
                    'source': 'papers-with-code',
                    'src_id': rid,
                    'best_rank': str(results_data[rid]['best_rank']),
                    'methodology': results_data[rid]['methodology'],
                    'pwc_evaluation_id': eval_id,
                    'pwc_dataset_id': pwc_dataset_id,
                    'pwc_task_id': pwc_task_id
            }
        metrics_data_list.append(data)
    print("Len of metrics data:", len(metrics_data_list))
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'metrics.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'metrics.csv')
    # write_to_csv(metrics_data_list, [filepath1, filepath2])



def create_frameworks():
    # KEYS - url, ownder, name, description, stars, framework, is_official
    related_data = json.load(open('/lustre/venkatre/data/pwc/paper-related.json', 'r'))
    # This will automatically remove duplicates of git repos as URL is set to be the ID
    new_data = {}
    for pid in related_data:
        git_repos = related_data[pid]['git-repos']
        if len(git_repos) == 0:
            pass
        else:
            for element in git_repos:
                url = element['url']
                new_data[url] = element

    # there are special characters at index 100000
    new_data['https://github.com/TATlong/keras_bert']['description'] = ''

    framework_data_list = []
    for url in new_data:
        framework_id = create_uuid_from_string(url)
        framework_name = new_data[url]['framework']
        desc = new_data[url]['description']
        desc = desc.replace(",", "")
        desc = desc.replace("\\", "")

        data = {'framework_id': framework_id,
                'framework_name': framework_name,
                'framework_version': framework_name,
                'source': 'papers-with-code',
                'url': url,
                'description': desc
                }
        framework_data_list.append(data)
    
    filepath1 = os.path.join(DEST_LUSTRE_PATH, 'frameworks.csv')
    filepath2 = os.path.join(DEST_LOCAL_PATH, 'frameworks.csv')
    write_to_csv(framework_data_list, [filepath1, filepath2])
    



# Function calls
create_nodes() # creates nodes for pipeline, stage, execution, artifact and report
create_tasks()
create_datasets()
create_models()
create_metrics()
create_frameworks()


