{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec3ec46-83e3-4eb9-817f-7896ea27f364",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Copyright (2024) Hewlett Packard Enterprise Development LP\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# You may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "print(\"Import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08223425-6db9-47a8-a455-d866cabca200",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URI = os.environ[\"NEO4J_URI\"]\n",
    "USER=os.environ[\"NEO4J_USER_NAME\"]\n",
    "PASSWORD=os.environ[\"NEO4J_PASSWD\"]\n",
    "AUTH = (os.environ[\"NEO4J_USER_NAME\"], os.environ[\"NEO4J_PASSWD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a873b4c9-abc7-42dd-903d-339353c3f0b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neo4J connect and Query Boilerplate\n",
    "\n",
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            #response = (session.run(query, parameters))\n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        \n",
    "        #return pd.DataFrame([r.values() for r in response], columns=response.keys())\n",
    "        return response\n",
    "    \n",
    "    def multi_query(self, multi_line_query, parameters=None, db=None):\n",
    "        for li in multi_line_query.splitlines():\n",
    "                print(li)\n",
    "                result=self.query(li, parameters=None, db=None)\n",
    "                print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2f780d-a930-40af-bdab-8ecabf60dcdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record count(n)=5304962>]\n"
     ]
    }
   ],
   "source": [
    "#Make a default connection and it should return `[<Record count(n)=0>]`\n",
    "conn = Neo4jConnection(uri=URI, \n",
    "                       user=USER,              \n",
    "                       pwd=PASSWORD)\n",
    "\n",
    "#if db is empty, then seed with init values \n",
    "res=conn.query('MATCH (n) RETURN count(n)')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b90b47-9881-41fa-b59c-d68f24ff0978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the constraint so that nodes with same IDs are merged. Execute this once per Database\n",
    "constraint_queries =[\n",
    "\"CREATE CONSTRAINT pipeline_id FOR (pipeline:Pipeline) REQUIRE pipeline.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT stage_id FOR (stage:Stage) REQUIRE stage.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT execution_id FOR (execution:Execution) REQUIRE execution.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT artifact_id FOR (artifact:Artifact) REQUIRE artifact.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT task_id FOR (task:Task) REQUIRE task.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT dataset_id FOR (dataset:Dataset) REQUIRE dataset.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT model_id FOR (model:Model) REQUIRE model.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT parameter_id FOR (parameter:Parameter) REQUIRE parameter.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT metric_id FOR (metric:Metric) REQUIRE metric.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT framework_id FOR (framework:Framework) REQUIRE framework.itemID IS UNIQUE;\",\n",
    "\"CREATE CONSTRAINT report_id FOR (report:Report) REQUIRE report.itemID IS UNIQUE;\",\n",
    "\"CALL db.awaitIndexes();\"\n",
    "]\n",
    "\n",
    "# for cquery in constraint_queries:\n",
    "#     try:\n",
    "#         res = conn.query(cquery)\n",
    "#         print(f\"Executed successfully: {cquery}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error executing query: {cquery}\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20131dfb-9de6-4a77-a65d-c315da7c0743",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query to load the nodes\n",
    "load_pipelines = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/pipelines.csv') yield map as row\", \n",
    "\"MERGE (pipeline:Pipeline {itemID: row.pipeline_id}) \n",
    "ON CREATE SET pipeline.name = row.pipeline_name, pipeline.source = row.source, pipeline.srcID = row.src_id\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_stages = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/stages.csv') yield map as row\", \n",
    "\"MERGE (stage:Stage {itemID: row.stage_id}) \n",
    "ON CREATE SET stage.name = row.stage_name, stage.source = row.source, stage.pipelineID = row.pipeline_id, stage.pipelineName = row.pipeline_name, stage.properties=row.properties\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_executions = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/executions.csv') yield map as row\", \n",
    "\"MERGE (execution:Execution {itemID: row.execution_id}) \n",
    "ON CREATE SET execution.name = row.execution_name, execution.srcID = row.src_id, execution.source = row.source, execution.pipelineID = row.pipelineID, execution.pipelineName = row.pipeline_name, execution.command = row.command, execution.properties=row.properties\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_artifacts = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/artifacts.csv') yield map as row\", \n",
    "\"MERGE (artifact:Artifact {itemID: row.artifact_id}) \n",
    "ON CREATE SET artifact.name = row.artifact_name, artifact.pipelineID = row.pipelineID, artifact.pipelineName = row.pipeline_name, artifact.source = row.source, artifact.executionID=row.execution_id\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_tasks = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/tasks.csv') yield map as row\", \n",
    "\"MERGE (task:Task {itemID: row.task_id}) \n",
    "ON CREATE SET task.name = row.task_name, task.source = row.source, task.taskDesc = row.task_description, task.srcID = row.src_id, task.taskType = row.task_type, task.modality = row.modality, task.category = row.category\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_datasets = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/datasets.csv') yield map as row\", \n",
    "\"MERGE (dataset:Dataset {itemID: row.dataset_id}) \n",
    "ON CREATE SET dataset.name = row.dataset_name, dataset.datasetDesc = row.description, dataset.srcID = row.src_id, dataset.url = row.url, dataset.modality=row.modality, dataset.source = row.source\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_models = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/models.csv') yield map as row\", \n",
    "\"MERGE (model:Model {itemID: row.model_id})\n",
    "ON CREATE SET model.name = row.model_name, model.modelClass = row.model_class, model.modelDesc = row.description, model.source = row.source, model.url=row.model_url\",\n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "# TODO - check parameters. Not loading properly. Some nan in csv\n",
    "load_params = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/parameters.csv') yield map as row\", \n",
    "\"MERGE (parameter:Parameter {itemID: row.parameter_id})\n",
    "ON CREATE SET parameter.setup=row.setup, parameter.settings=row.settings, parameter.source = row.source\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "# There are a lot of properties for eval node. Loading only some for now. All these measures should be under metric property or custom property\n",
    "load_metrics = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/metrics.csv') yield map as row\", \n",
    "\"MERGE (metric:Metric {itemID: row.metric_id}) \n",
    "ON CREATE SET metric.result=row.metrics\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_frameworks = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/frameworks.csv') yield map as row\", \n",
    "\"MERGE (framework:Framework {itemID: row.framework_id})\n",
    "ON CREATE SET framework.name = row.framework_name, framework.url = row.url, framework.description=row.description, framework.version = row.framework_version, framework.source = row.source\",\n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "# no reports file in the location - openml removed it in their new format\n",
    "load_reports = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/nodes/reports.csv') yield map as row\", \n",
    "\"MERGE (report:Report {itemID: row.report_id})\n",
    "ON CREATE SET report.name = row.title, report.url = row.url, report.abstractUrl = row.abstract_url, report.source = row.source, report.srcID = row.src_id\",\n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04be0ed0-5c73-44c2-bba9-d9659b8714fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Queries to load relationships\n",
    "rel_pipeline_task = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-pipeline-task.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (task:Task {itemID: row.task_id})\n",
    "MERGE (pipeline)-[:executes]->(task)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_pipeline_framework = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-pipeline-framework.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (framework:Framework {itemID: row.framework_id})\n",
    "MERGE (pipeline)-[:uses]->(framework)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_pipeline_report = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-pipeline-report.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (report:Report {itemID: row.report_id})\n",
    "MERGE (pipeline)-[:has]->(report)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_pipeline_stage = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-pipeline-stage.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (stage:Stage {itemID: row.stage_id})\n",
    "MERGE (pipeline)-[:contains]->(stage)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_stage_execution = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-stage-execution.csv') yield map as row\", \n",
    "\"MATCH (stage:Stage {itemID: row.stage_id})\n",
    "MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MERGE (stage)-[:runs]->(execution)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_execution_metric = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-execution-metric.csv') yield map as row\", \n",
    "\"MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MATCH (metric:Metric {itemID: row.metric_id})\n",
    "MERGE (execution)-[:generates]->(metric)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_execution_artifact = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-execution-artifact.csv') yield map as row\", \n",
    "\"MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MERGE (execution)-[:isOutput]->(artifact)\n",
    "MERGE (execution)<-[:isInput]-(artifact)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_artifact_dataset = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-artifact-dataset.csv') yield map as row\", \n",
    "\"MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MATCH (dataset:Dataset {itemID: row.dataset_id})\n",
    "MERGE (artifact)<-[:subCatOf]-(dataset)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_artifact_model = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-artifact-model.csv') yield map as row\", \n",
    "\"MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MATCH (model:Model {itemID: row.model_id})\n",
    "MERGE (artifact)<-[:subCatOf]-(model)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_artifact_metric = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-artifact-metric.csv') yield map as row\", \n",
    "\"MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MATCH (metric:Metric {itemID: row.metric_id})\n",
    "MERGE (artifact)<-[:subCatOf]-(metric)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_model_parameter = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/open-ml/relationships/rel-model-parameter.csv') yield map as row\", \n",
    "\"MATCH (model:Model {itemID: row.model_id})\n",
    "MATCH (parameter:Parameter {itemID: row.parameter_id})\n",
    "MERGE (model)<-[:uses]-(parameter)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2af2d1ac-e0dd-49d5-b3ac-37f2288f8c29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Nodes...\n",
      "Pipeline nodes loaded. Time taken:3 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Stage nodes loaded. Time taken:3 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Execution nodes loaded. Time taken:5 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Artifacts nodes loaded. Time taken:5 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Task nodes loaded. Time taken:0 seconds. Committed Operations: 46592. Failed Operations:0\n",
      "Dataset nodes loaded. Time taken:0 seconds. Committed Operations: 3336. Failed Operations:0\n",
      "Model nodes loaded. Time taken:0 seconds. Committed Operations: 16269. Failed Operations:0\n",
      "Metrics nodes loaded. Time taken:4 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Framework nodes loaded. Time taken:2 seconds. Committed Operations: 575826. Failed Operations:0\n",
      "--- 24.93839454650879 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# NOTE - constraints is to be executed only once per KG. If it already executed as a part of loading PWC or HF dataset, this does not need to be executed\n",
    "# const_res = conn.query(constraint)\n",
    "# call_db_res = conn.query(call_db)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# Loading nodes to KG\n",
    "print(\"Adding Nodes...\")\n",
    "\n",
    "res_pipeline=conn.query(load_pipelines)\n",
    "print(\"Pipeline nodes loaded. Time taken:\" + str(res_pipeline[0][2]) + ' seconds. Committed Operations: ' + str(res_pipeline[0][3]) + '. Failed Operations:' + str(res_pipeline[0][4]))\n",
    "\n",
    "res_stages=conn.query(load_stages)\n",
    "print(\"Stage nodes loaded. Time taken:\" + str(res_stages[0][2]) + ' seconds. Committed Operations: ' + str(res_stages[0][3]) + '. Failed Operations:' + str(res_stages[0][4]))\n",
    "\n",
    "res_executions=conn.query(load_executions)\n",
    "print(\"Execution nodes loaded. Time taken:\" + str(res_executions[0][2]) + ' seconds. Committed Operations: ' + str(res_executions[0][3]) + '. Failed Operations:' + str(res_executions[0][4]))\n",
    "\n",
    "res_artifacts=conn.query(load_artifacts)\n",
    "print(\"Artifacts nodes loaded. Time taken:\" + str(res_artifacts[0][2]) + ' seconds. Committed Operations: ' + str(res_artifacts[0][3]) + '. Failed Operations:' + str(res_artifacts[0][4]))\n",
    "\n",
    "res_tasks=conn.query(load_tasks)\n",
    "print(\"Task nodes loaded. Time taken:\" + str(res_tasks[0][2]) + ' seconds. Committed Operations: ' + str(res_tasks[0][3]) + '. Failed Operations:' + str(res_tasks[0][4]))\n",
    "\n",
    "res_datasets=conn.query(load_datasets)\n",
    "print(\"Dataset nodes loaded. Time taken:\" + str(res_datasets[0][2]) + ' seconds. Committed Operations: ' + str(res_datasets[0][3]) + '. Failed Operations:' + str(res_datasets[0][4]))\n",
    "\n",
    "res_models=conn.query(load_models)\n",
    "print(\"Model nodes loaded. Time taken:\" + str(res_models[0][2]) + ' seconds. Committed Operations: ' + str(res_models[0][3]) + '. Failed Operations:' + str(res_models[0][4]))\n",
    "\n",
    "# Some problem with metric nodes. They are not being laoded.\n",
    "# res_params=conn.query(load_params)\n",
    "# print(\"Parameter nodes loaded. Time taken:\" + str(res_params[0][2]) + ' seconds. Committed Operations: ' + str(res_params[0][3]) + '. Failed Operations:' + str(res_params[0][4]))\n",
    "\n",
    "res_metrics=conn.query(load_metrics)\n",
    "print(\"Metrics nodes loaded. Time taken:\" + str(res_metrics[0][2]) + ' seconds. Committed Operations: ' + str(res_metrics[0][3]) + '. Failed Operations:' + str(res_metrics[0][4]))\n",
    "\n",
    "res_frameworks=conn.query(load_frameworks)\n",
    "print(\"Framework nodes loaded. Time taken:\" + str(res_frameworks[0][2]) + ' seconds. Committed Operations: ' + str(res_frameworks[0][3]) + '. Failed Operations:' + str(res_frameworks[0][4]))\n",
    "\n",
    "# No report in new openml data\n",
    "# res_reports=conn.query(load_reports)\n",
    "# print(\"Report nodes loaded. Time taken:\" + str(res_reports[0][2]) + ' seconds. Committed Operations: ' + str(res_reports[0][3]) + '. Failed Operations:' + str(res_reports[0][4]))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca2e894b-c9b8-4bec-9d6f-69474c48afc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Relationships..\n",
      "Added relationship between Pipeline and Framework. Time taken:1 seconds. Committed Operations: 575826. Failed Operations:0\n",
      "Added relationship between Pipeline and Task. Time taken:1 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Added relationship between Pipeline and Stage. Time taken:1 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Added relationship between Stage and Execution. Time taken:1 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Added relationship between Execution and Metric. Time taken:1 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Added relationship between Execution and Artifact. Time taken:1 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Added relationship between Artifact and Dataset. Time taken:1 seconds. Committed Operations: 673320. Failed Operations:0\n",
      "Added relationship between Artifact and Model. Time taken:1 seconds. Committed Operations: 669114. Failed Operations:0\n",
      "Added relationship between Artifact and Metric. Time taken:1 seconds. Committed Operations: 673359. Failed Operations:0\n",
      "Added relationship between Model and Parameter. Time taken:1 seconds. Committed Operations: 669114. Failed Operations:0\n",
      "--- 11.872364282608032 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# NOTE - IMPORTANT\n",
    "# If the result of some query has failed operations, copy paste the query in neo4j browser and run it again and again till the value under \n",
    "#\"Failed Operations\" become 0.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Loading relationships\n",
    "print(\"Adding Relationships..\")\n",
    "res = conn.query(rel_pipeline_framework)\n",
    "print(\"Added relationship between Pipeline and Framework. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_pipeline_task)\n",
    "print(\"Added relationship between Pipeline and Task. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_pipeline_stage)\n",
    "print(\"Added relationship between Pipeline and Stage. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_stage_execution)\n",
    "print(\"Added relationship between Stage and Execution. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_execution_metric)\n",
    "print(\"Added relationship between Execution and Metric. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_execution_artifact)\n",
    "print(\"Added relationship between Execution and Artifact. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_artifact_dataset)\n",
    "print(\"Added relationship between Artifact and Dataset. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_artifact_model)\n",
    "print(\"Added relationship between Artifact and Model. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_artifact_metric)\n",
    "print(\"Added relationship between Artifact and Metric. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_model_parameter)\n",
    "print(\"Added relationship between Model and Parameter. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "# No report in new openml data\n",
    "# res = conn.query(rel_pipeline_report)\n",
    "# print(\"Added relationship between Pipeline and Report. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d58ca6-8a11-40de-a89d-9e4484f741ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654eae23-6a11-4e9b-aebe-2a1d3c8c0e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
