{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "559f3f5c-bdbd-45b1-8223-edc4db35b1db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# Copyright (2024) Hewlett Packard Enterprise Development LP\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# You may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "print(\"Import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24894c64-61cf-4e92-adb4-362d2587ea32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URI = os.environ[\"NEO4J_URI\"]\n",
    "USER=os.environ[\"NEO4J_USER_NAME\"]\n",
    "PASSWORD=os.environ[\"NEO4J_PASSWD\"]\n",
    "AUTH = (os.environ[\"NEO4J_USER_NAME\"], os.environ[\"NEO4J_PASSWD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc39e1f6-e219-44e7-a8b8-770b2d847090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neo4J connect and Query Boilerplate\n",
    "\n",
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            #response = (session.run(query, parameters))\n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        \n",
    "        #return pd.DataFrame([r.values() for r in response], columns=response.keys())\n",
    "        return response\n",
    "    \n",
    "    def multi_query(self, multi_line_query, parameters=None, db=None):\n",
    "        for li in multi_line_query.splitlines():\n",
    "                print(li)\n",
    "                result=self.query(li, parameters=None, db=None)\n",
    "                print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06003a94-533d-4032-8079-e7cc5503ad7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record count(n)=7356314>]\n"
     ]
    }
   ],
   "source": [
    "#Make a default connection and it should return `[<Record count(n)=0>]`\n",
    "conn = Neo4jConnection(uri=URI, \n",
    "                       user=USER,              \n",
    "                       pwd=PASSWORD)\n",
    "\n",
    "#if db is empty, then seed with init values \n",
    "res=conn.query('MATCH (n) RETURN count(n)')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5589ec37-3fd8-4ff4-9734-724913924083",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE - to be executed only once per KG. If it alreafy executed as a part of loading PWC dataset, this does not need to be executed\n",
    "# Before adding relationships, need to construct a constraint\n",
    "constraint = \"\"\"\n",
    "CREATE CONSTRAINT pipeline_id FOR (pipeline:Pipeline) REQUIRE pipeline.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT stage_id FOR (stage:Stage) REQUIRE stage.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT execution_id FOR (execution:Execution) REQUIRE execution.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT artifact_id FOR (artifact:Artifact) REQUIRE artifact.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT task_id FOR (task:Task) REQUIRE task.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT dataset_id FOR (dataset:Dataset) REQUIRE dataset.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT model_id FOR (model:Model) REQUIRE model.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT parameter_id FOR (parameter:Parameter) REQUIRE parameter.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT metric_id FOR (metric:Metric) REQUIRE metric.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT framework_id FOR (framework:Framework) REQUIRE framework.itemID IS UNIQUE;\n",
    "CREATE CONSTRAINT report_id FOR (report:Report) REQUIRE report.itemID IS UNIQUE;\n",
    "CALL db.awaitIndexes();\n",
    "\"\"\"\n",
    "# TODO - parameters need to be added to constraints\n",
    "\n",
    "# DROP INDEX ON :Dataset(datasetID)\n",
    "\n",
    "call_db = \"\"\"CALL db.indexes();\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d68ad5-a285-4a5e-8eb9-cbf04ed63277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query to load the nodes\n",
    "load_pipelines = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/pipelines.csv') yield map as row\", \n",
    "\"MERGE (pipeline:Pipeline {itemID: row.pipeline_id}) \n",
    "ON CREATE SET pipeline.name = row.pipeline_name, pipeline.source = row.source, pipeline.srcID = row.src_id\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_stages = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/stages.csv') yield map as row\", \n",
    "\"MERGE (stage:Stage {itemID: row.stage_id}) \n",
    "ON CREATE SET stage.name = row.stage_name, stage.source = row.source, stage.pipelineID = row.pipeline_id, stage.pipelineName = row.pipeline_name, stage.properties=row.properties\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_executions = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/executions.csv') yield map as row\", \n",
    "\"MERGE (execution:Execution {itemID: row.execution_id}) \n",
    "ON CREATE SET execution.name = row.execution_name, execution.srcID = row.src_id, execution.source = row.source, execution.pipelineID = row.pipelineID, execution.pipelineName = row.pipeline_name, execution.command = row.command, execution.properties=row.properties\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_artifacts = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/artifacts.csv') yield map as row\", \n",
    "\"MERGE (artifact:Artifact {itemID: row.artifact_id}) \n",
    "ON CREATE SET artifact.name = row.artifact_name, artifact.pipelineID = row.pipelineID, artifact.pipelineName = row.pipeline_name, artifact.source = row.source, artifact.executionID=row.execution_id\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_tasks = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/tasks.csv') yield map as row\", \n",
    "\"MERGE (task:Task {itemID: row.task_id}) \n",
    "ON CREATE SET task.name = row.task_name, task.source = row.source, task.taskDesc = row.task_description, task.srcID = row.src_id, task.taskType = row.task_type, task.modality = row.modality, task.category = row.category\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_datasets = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/datasets.csv') yield map as row\", \n",
    "\"MERGE (dataset:Dataset {itemID: row.dataset_id}) \n",
    "ON CREATE SET dataset.name = row.dataset_name, dataset.datasetDesc = row.description, dataset.srcID = row.src_id, dataset.url = row.url, dataset.modality=row.modality, dataset.source = row.source\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_models = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/models.csv') yield map as row\", \n",
    "\"MERGE (model:Model {itemID: row.model_id})\n",
    "ON CREATE SET model.name = row.model_name, model.modelClass = row.model_class, model.url = row.model_url, model.modelDesc = row.description, model.source = row.source\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_metrics = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/metrics.csv') yield map as row\", \n",
    "\"MERGE (metric:Metric {itemID: row.metric_id}) \n",
    "ON CREATE SET metric.result=row.metrics, metric.source = row.source\", \n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_frameworks = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/frameworks.csv') yield map as row\", \n",
    "\"MERGE (framework:Framework {itemID: row.framework_id})\n",
    "ON CREATE SET framework.name = row.framework_name, framework.url = row.url, framework.description = row.description, framework.version = row.framework_version, framework.source = row.source\",\n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "load_reports = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/nodes/reports.csv') yield map as row\", \n",
    "\"MERGE (report:Report {itemID: row.report_id})\n",
    "ON CREATE SET report.name = row.title, report.url = row.url, report.abstractUrl = row.abstract_url, report.source = row.source, report.srcID = row.src_id\",\n",
    "{batchSize:1000, iterateList:true, parallel:true})\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8238d2a-c93b-472b-b482-2d0f90c8fd90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Queries to load relationships\n",
    "rel_pipeline_task = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-pipeline-task.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (task:Task {itemID: row.task_id})\n",
    "MERGE (pipeline)-[:executes]->(task)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_pipeline_framework = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-pipeline-framework.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (framework:Framework {itemID: row.framework_id})\n",
    "MERGE (pipeline)-[:uses]->(framework)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_pipeline_report = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-pipeline-report.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (report:Report {itemID: row.report_id})\n",
    "MERGE (pipeline)-[:has]->(report)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_pipeline_stage = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-pipeline-stage.csv') yield map as row\", \n",
    "\"MATCH (pipeline:Pipeline {itemID: row.pipeline_id})\n",
    "MATCH (stage:Stage {itemID: row.stage_id})\n",
    "MERGE (pipeline)-[:contains]->(stage)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_stage_execution = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-stage-execution.csv') yield map as row\", \n",
    "\"MATCH (stage:Stage {itemID: row.stage_id})\n",
    "MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MERGE (stage)-[:runs]->(execution)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_execution_metric = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-execution-metric.csv') yield map as row\", \n",
    "\"MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MATCH (metric:Metric {itemID: row.metric_id})\n",
    "MERGE (execution)-[:generates]->(metric)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_execution_artifact = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-execution-artifact.csv') yield map as row\", \n",
    "\"MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MERGE (execution)-[:isOutput]->(artifact)\n",
    "MERGE (execution)<-[:isInput]-(artifact)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_artifact_dataset = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-artifact-dataset.csv') yield map as row\", \n",
    "\"MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MATCH (dataset:Dataset {itemID: row.dataset_id})\n",
    "MERGE (artifact)<-[:subCatOf]-(dataset)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_artifact_model = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-artifact-model.csv') yield map as row\", \n",
    "\"MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MATCH (model:Model {itemID: row.model_id})\n",
    "MERGE (artifact)<-[:subCatOf]-(model)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_artifact_metric = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-artifact-metric.csv') yield map as row\", \n",
    "\"MATCH (artifact:Artifact {itemID: row.artifact_id})\n",
    "MATCH (metric:Metric {itemID: row.metric_id})\n",
    "MERGE (artifact)<-[:subCatOf]-(metric)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\"\n",
    "\n",
    "rel_execution_metric = \"\"\"CALL apoc.periodic.iterate(\"CALL apoc.load.csv('/huggingface/relationships/rel-execution-metric.csv') yield map as row\", \n",
    "\"MATCH (execution:Execution {itemID: row.execution_id})\n",
    "MATCH (metric:Metric {itemID: row.metric_id})\n",
    "MERGE (execution)-[:generates]->(metric)\", \n",
    "{batchSize:10000, iterateList:true, parallel:true})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d43f379b-4716-4521-826c-e39eef6629c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Nodes...\n",
      "Pipeline nodes loaded. Time taken:1 seconds. Committed Operations: 267140. Failed Operations:0\n",
      "Stage nodes loaded. Time taken:1 seconds. Committed Operations: 267140. Failed Operations:0\n",
      "Execution nodes loaded. Time taken:1 seconds. Committed Operations: 287189. Failed Operations:0\n",
      "Artifacts nodes loaded. Time taken:2 seconds. Committed Operations: 287189. Failed Operations:0\n",
      "Task nodes loaded. Time taken:0 seconds. Committed Operations: 40. Failed Operations:0\n",
      "Dataset nodes loaded. Time taken:0 seconds. Committed Operations: 47318. Failed Operations:0\n",
      "Model nodes loaded. Time taken:1 seconds. Committed Operations: 267140. Failed Operations:0\n",
      "Metrics nodes loaded. Time taken:0 seconds. Committed Operations: 64421. Failed Operations:0\n",
      "Framework nodes loaded. Time taken:0 seconds. Committed Operations: 267140. Failed Operations:0\n",
      "Report nodes loaded. Time taken:0 seconds. Committed Operations: 467. Failed Operations:0\n",
      "--- 10.68008804321289 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# NOTE - constraints is to be executed only once per KG. If it already executed as a part of loading other data sources, this does not need to be executed\n",
    "# const_res = conn.query(constraint)\n",
    "# call_db_res = conn.query(call_db)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "# Loading nodes to KG\n",
    "print(\"Adding Nodes...\")\n",
    "\n",
    "res_pipeline=conn.query(load_pipelines)\n",
    "print(\"Pipeline nodes loaded. Time taken:\" + str(res_pipeline[0][2]) + ' seconds. Committed Operations: ' + str(res_pipeline[0][3]) + '. Failed Operations:' + str(res_pipeline[0][4]))\n",
    "\n",
    "res_stages=conn.query(load_stages)\n",
    "print(\"Stage nodes loaded. Time taken:\" + str(res_stages[0][2]) + ' seconds. Committed Operations: ' + str(res_stages[0][3]) + '. Failed Operations:' + str(res_stages[0][4]))\n",
    "\n",
    "res_executions=conn.query(load_executions)\n",
    "print(\"Execution nodes loaded. Time taken:\" + str(res_executions[0][2]) + ' seconds. Committed Operations: ' + str(res_executions[0][3]) + '. Failed Operations:' + str(res_executions[0][4]))\n",
    "\n",
    "res_artifacts=conn.query(load_artifacts)\n",
    "print(\"Artifacts nodes loaded. Time taken:\" + str(res_artifacts[0][2]) + ' seconds. Committed Operations: ' + str(res_artifacts[0][3]) + '. Failed Operations:' + str(res_artifacts[0][4]))\n",
    "\n",
    "res_tasks=conn.query(load_tasks)\n",
    "print(\"Task nodes loaded. Time taken:\" + str(res_tasks[0][2]) + ' seconds. Committed Operations: ' + str(res_tasks[0][3]) + '. Failed Operations:' + str(res_tasks[0][4]))\n",
    "\n",
    "res_datasets=conn.query(load_datasets)\n",
    "print(\"Dataset nodes loaded. Time taken:\" + str(res_datasets[0][2]) + ' seconds. Committed Operations: ' + str(res_datasets[0][3]) + '. Failed Operations:' + str(res_datasets[0][4]))\n",
    "\n",
    "res_models=conn.query(load_models)\n",
    "print(\"Model nodes loaded. Time taken:\" + str(res_models[0][2]) + ' seconds. Committed Operations: ' + str(res_models[0][3]) + '. Failed Operations:' + str(res_models[0][4]))\n",
    "\n",
    "res_metrics=conn.query(load_metrics)\n",
    "print(\"Metrics nodes loaded. Time taken:\" + str(res_metrics[0][2]) + ' seconds. Committed Operations: ' + str(res_metrics[0][3]) + '. Failed Operations:' + str(res_metrics[0][4]))\n",
    "\n",
    "res_frameworks=conn.query(load_frameworks)\n",
    "print(\"Framework nodes loaded. Time taken:\" + str(res_frameworks[0][2]) + ' seconds. Committed Operations: ' + str(res_frameworks[0][3]) + '. Failed Operations:' + str(res_frameworks[0][4]))\n",
    "\n",
    "res_reports=conn.query(load_reports)\n",
    "print(\"Report nodes loaded. Time taken:\" + str(res_reports[0][2]) + ' seconds. Committed Operations: ' + str(res_reports[0][3]) + '. Failed Operations:' + str(res_reports[0][4]))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb73e729-b628-4798-95a8-d4c396a54dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Relationships..\n",
      "Added relationship between Pipeline and Framework. Time taken:3 seconds. Committed Operations: 207140. Failed Operations:60000\n",
      "Added relationship between Pipeline and Task. Time taken:0 seconds. Committed Operations: 88907. Failed Operations:60000\n",
      "Added relationship between Pipeline and Stage. Time taken:0 seconds. Committed Operations: 267140. Failed Operations:0\n",
      "Added relationship between Stage and Execution. Time taken:0 seconds. Committed Operations: 287189. Failed Operations:0\n",
      "Added relationship between Execution and Artifact. Time taken:0 seconds. Committed Operations: 287189. Failed Operations:0\n",
      "Added relationship between Artifact and Dataset. Time taken:0 seconds. Committed Operations: 36712. Failed Operations:0\n",
      "Added relationship between Artifact and Model. Time taken:0 seconds. Committed Operations: 287189. Failed Operations:0\n",
      "Added relationship between Artifact and Metric. Time taken:0 seconds. Committed Operations: 64269. Failed Operations:0\n",
      "Added relationship between Execution and Metric. Time taken:0 seconds. Committed Operations: 64269. Failed Operations:0\n",
      "Added relationship between Pipeline and Report. Time taken:0 seconds. Committed Operations: 8246. Failed Operations:0\n",
      "--- 7.917020559310913 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "# NOTE - IMPORTANT\n",
    "# If the result of some query has failed operations, copy paste the query in neo4j browser and run it again and again till the value under \n",
    "#\"Failed Operations\" become 0.\n",
    "\"\"\"\n",
    "\n",
    "start_time = time.time()\n",
    "# Loading relationships\n",
    "print(\"Adding Relationships..\")\n",
    "res = conn.query(rel_pipeline_framework)\n",
    "print(\"Added relationship between Pipeline and Framework. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_pipeline_task)\n",
    "print(\"Added relationship between Pipeline and Task. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_pipeline_stage)\n",
    "print(\"Added relationship between Pipeline and Stage. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_stage_execution)\n",
    "print(\"Added relationship between Stage and Execution. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_execution_artifact)\n",
    "print(\"Added relationship between Execution and Artifact. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_artifact_dataset)\n",
    "print(\"Added relationship between Artifact and Dataset. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_artifact_model)\n",
    "print(\"Added relationship between Artifact and Model. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_artifact_metric)\n",
    "print(\"Added relationship between Artifact and Metric. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_execution_metric)\n",
    "print(\"Added relationship between Execution and Metric. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "res = conn.query(rel_pipeline_report)\n",
    "print(\"Added relationship between Pipeline and Report. Time taken:\" + str(res[0][2]) + ' seconds. Committed Operations: ' + str(res[0][3]) + '. Failed Operations:' + str(res[0][4]))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1c52f-de54-4969-99b8-0d1a64004c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07455b44-2f12-4450-b7ea-ba7dbfef6172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
